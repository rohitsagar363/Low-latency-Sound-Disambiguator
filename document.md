# ğŸ§ Low-Latency Sound Disambiguator

<div align="center">

> Real-Time Audio Intelligence Dashboard for Sound Awareness

[![Hackathon Track](https://img.shields.io/badge/Track-AI%20for%20Accessibility-blue)](https://github.com/rohitsagar363/Low-latency-Sound-Disambiguator)
[![Edge Intelligence](https://img.shields.io/badge/Technology-Edge%20Intelligence-green)](https://github.com/rohitsagar363/Low-latency-Sound-Disambiguator)

</div>

## ğŸ¯ Mission

Empowering deaf and hearing-impaired individuals with real-time sound awareness through AI-powered visual alerts.

## ğŸ§© Overview

The **Low-Latency Sound Disambiguator** transforms environmental sounds into instant visual alerts, making the auditory world accessible to everyone. Our system:

- ğŸ¤ **Captures** continuous audio input in real-time
- ğŸ¤– **Analyzes** sounds using Google's YAMNet ML model
- ğŸ§  **Interprets** context through Ollama (Mistral) AI
- ğŸ“Š **Visualizes** alerts through an intuitive dashboard

## ğŸš¨ Use Case Example: Police Siren Detection

<div align="center">

### Without Sound Disambiguator
<img src="/images/before_siren.png" alt="Without System" width="600"/>

*A deaf person unable to hear approaching emergency vehicle sirens*

### With Sound Disambiguator
<img src="/images/with_siren.png" alt="With System" width="600"/>

*Real-time visual alert showing:*
- ğŸš“ **Detection**: Police siren detected
- ğŸ“ **Direction**: Coming from behind, ~100m away
- ğŸ”Š **Intensity**: High (Emergency vehicle approaching)
- âš ï¸ **Action Required**: Move to the side of the road

</div>

## ğŸ“Š Dashboard Interface

### ğŸ¯ Live Tab
<img src="/images/live_tab.png" alt="Live Dashboard" width="800"/>

*Real-time monitoring and detection interface*
- Sound classification with confidence levels
- Direction indicator with spatial awareness
- Color-coded alert banner system
- Live AI interpretations of detected sounds

### ğŸ“œ History Tab
<img src="/images/history_tab.png" alt="History View" width="800"/>

*Historical data and event tracking*
- Chronological log of detected sounds
- Time-stamped events with classifications
- Filter and search functionality
- Export capabilities for analysis

### ğŸ“ˆ Analytics Tab
<img src="/images/analytics_tab.png" alt="Analytics Dashboard" width="800"/>

*Statistical analysis and insights*
- Sound type distribution charts
- Temporal pattern analysis
- Alert frequency statistics
- Performance metrics visualization

### ğŸ§  Insights Tab
<img src="/images/insights_tab.png" alt="AI Insights" width="800"/>

*AI-powered interpretation and recommendations*
- Contextual sound interpretations
- Pattern recognition summaries
- Environmental safety scoring
- Actionable safety recommendations

## ğŸ—ï¸ System Architecture

```mermaid
graph TD
    A[ğŸ¤ Microphone Input] --> B[SoundDevice Stream]
    B --> C[YAMNet Model]
    C --> D{Sound Classification}
    D -->|Confidence & Label| E[Live Dashboard]
    D -->|Events| H[Analytics Engine]
    E --> F[Ollama Mistral]
    F --> G[Insights Generation]
    H --> I[Historical Data]
    E --> J[Alert System]
    J -->|Status| K[Visual Indicators]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#bbf,stroke:#333,stroke-width:2px
    style J fill:#fbb,stroke:#333,stroke-width:2px
```

## ğŸ§° Technology Stack

| Layer | Components | Description |
|-------|------------|-------------|
| ğŸ¨ Frontend | Streamlit, Plotly | Interactive dashboard with real-time updates |
| ğŸµ Audio | SoundDevice, NumPy | High-performance audio stream processing |
| ğŸ¤– ML/AI | TensorFlow Hub, YAMNet | Sound classification and analysis |
| ğŸ§  Intelligence | Ollama (Mistral) | Local LLM for context interpretation |
| ğŸ”„ Processing | Threading, Queue | Concurrent operation handling |
| ğŸ“Š Visualization | Plotly, Custom CSS | Dynamic charts and alert banners |

The **Low-Latency Sound Disambiguator** represents an accessible, intelligent, and privacy-respecting approach to real-time audio awareness. It's lightweight, responsive, and extendable â€” bridging AI, accessibility, and edge computing into one unified platform..png)guator

> Real-Time Audio Intelligence Dashboard for Sound Awareness

[![Hackathon Track](https://img.shields.io/badge/Track-AI%20for%20Accessibility-blue)](https://github.com/rohitsagar363/Low-latency-Sound-Disambiguator)
[![Edge Intelligence](https://img.shields.io/badge/Technology-Edge%20Intelligence-green)](https://github.com/rohitsagar363/Low-latency-Sound-Disambiguator)

## ğŸ§© Overview

The **Low-Latency Sound Disambiguator** is an innovative real-time audio intelligence system designed to enhance accessibility and sound awareness. This project:

- ğŸ¯ Listens to live microphone input continuously
- ğŸ¤– Utilizes Google's **YAMNet** for precise sound detection
- ğŸ§  Leverages **Ollama (Mistral)** for intelligent sound interpretation
- ğŸ“Š Provides instant visual feedback through an intuitive dashboard

### Core Use Case

> Empowering deaf and hearing-impaired individuals by transforming critical sounds (alarms, sirens, shouts) into clear, real-time visual alerts, enhancing safety and environmental awareness.

## ï¿½ Use Case Example: Police Siren Detection

<div align="center">

### Without Sound Disambiguator
![Without System](/images/before_siren.png)
*A deaf person unable to hear approaching emergency vehicle sirens*

### With Sound Disambiguator
![With System](/images/with_siren.png)
*Real-time visual alert showing:*
- ğŸš“ **Detection**: Police siren detected
- ğŸ“ **Direction**: Coming from behind, ~100m away
- ğŸ”Š **Intensity**: High (Emergency vehicle approaching)
- âš ï¸ **Action Required**: Move to the side of the road

</div>

> **Note**: Create an `images` directory and add relevant screenshots or mockups of your system in action. The images above are placeholders - replace them with actual screenshots from your application showing the alert system responding to police sirens.

## ï¿½ğŸš€ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ” Real-time Classification | Advanced sound detection using YAMNet (TensorFlow Hub) |
| âš¡ Low-latency Processing | Efficient streaming with SoundDevice + async queue |
| ğŸ¤– AI Insights | Smart context interpretation via Ollama (Mistral) |
| ğŸ“¡ Direction Detection | Intelligent stereo microphone utilization with fallback simulation |
| ğŸš¦ Visual Alerts | Smart color-coding system: ğŸŸ¢ Safe â€¢ ğŸŸ¡ Neutral â€¢ ğŸ”´ Alert |
| ğŸ“Š Rich Dashboard | Comprehensive view with Live/History/Analytics/Insights tabs |
| ğŸ’¨ Lightweight Design | Quick local deployment with minimal resource usage |  

## ğŸ“Š Dashboard Interface

### ğŸ¯ Live Tab
![Live Dashboard](/images/live_tab.png)
*Real-time monitoring and detection interface*
- Sound classification with confidence levels
- Direction indicator with spatial awareness
- Color-coded alert banner system
- Live AI interpretations of detected sounds

### ğŸ“œ History Tab
![History View](/images/history_tab.png)
*Historical data and event tracking*
- Chronological log of detected sounds
- Time-stamped events with classifications
- Filter and search functionality
- Export capabilities for analysis

### ğŸ“ˆ Analytics Tab
![Analytics Dashboard](/images/analytics_tab.png)
*Statistical analysis and insights*
- Sound type distribution charts
- Temporal pattern analysis
- Alert frequency statistics
- Performance metrics visualization

### ğŸ§  Insights Tab
![AI Insights](/images/insights_tab.png)
*AI-powered interpretation and recommendations*
- Contextual sound interpretations
- Pattern recognition summaries
- Environmental safety scoring
- Actionable safety recommendations

> **ğŸ“¸ Screenshots:** Place your application screenshots in `/images/`:
> - `live_tab.png` - Main monitoring interface
> - `history_tab.png` - Historical data view
> - `analytics_tab.png` - Statistical analysis view
> - `insights_tab.png` - AI interpretation view
> - `before_siren.png` - Use case without system
> - `with_siren.png` - Use case with system active

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TD
    A[ğŸ¤ Microphone Input] --> B[SoundDevice Stream]
    B --> C[YAMNet Model]
    C --> D{Sound Classification}
    D -->|Confidence & Label| E[Live Dashboard]
    D -->|Events| H[Analytics Engine]
    E --> F[Ollama Mistral]
    F --> G[Insights Generation]
    H --> I[Historical Data]
    E --> J[Alert System]
    J -->|Status| K[Visual Indicators]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#bbf,stroke:#333,stroke-width:2px
    style J fill:#fbb,stroke:#333,stroke-width:2px
```

## ğŸ§° Technology Stack

| Layer | Components | Description |
|-------|------------|-------------|
| ğŸ¨ Frontend | Streamlit, Plotly | Interactive dashboard with real-time updates |
| ğŸµ Audio | SoundDevice, NumPy | High-performance audio stream processing |
| ğŸ¤– ML/AI | TensorFlow Hub, YAMNet | Sound classification and analysis |
| ğŸ§  Intelligence | Ollama (Mistral) | Local LLM for context interpretation |
| ğŸ”„ Processing | Threading, Queue | Concurrent operation handling |
| ğŸ“Š Visualization | Plotly, Custom CSS | Dynamic charts and alert banners |

## âš™ï¸ Installation & Setup

### 1. Clone the Repository

```bash
git clone https://github.com/rohitsagar363/Low-latency-Sound-Disambiguator.git
cd Low-latency-Sound-Disambiguator
```

### 2. Set Up Python Environment

```bash
# Create virtual environment
python3 -m venv venv

# Activate virtual environment
# For Unix/macOS:
source venv/bin/activate
# For Windows:
# venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

#### Required Packages
```plaintext
streamlit>=1.24.0
tensorflow>=2.12.0
tensorflow_hub>=0.14.0
sounddevice>=0.4.6
numpy>=1.23.5
plotly>=5.15.0
requests>=2.31.0
pandas>=2.0.3
```

### 4. Configure Ollama

1. Download Ollama from [ollama.ai](https://ollama.ai)
2. Start the Ollama service:
```bash
# Pull the Mistral model
ollama pull mistral

# Start the Ollama service
ollama serve
```

### 5. Launch the Dashboard

```bash
streamlit run sound_alert_appv3.py
```

> ğŸ¤ **Note:** Grant microphone access when prompted by your system.
ğŸ”Š Grant microphone access when prompted.

## ğŸ§  System Operation

### Real-time Processing Pipeline

1. **Audio Capture**
   - Continuous streaming in 1.5-second chunks
   - Overlapping segments for smooth analysis
   - Real-time buffer management

2. **Sound Classification**
   - YAMNet model processes each chunk
   - Identifies sound categories and confidence levels
   - Low-latency inference optimization

3. **AI Interpretation**
   - Ollama (Mistral) analyzes classification results
   - Generates contextual two-line summaries
   - Provides human-readable insights

4. **Spatial Analysis**
   - Stereo microphone direction estimation
   - Fallback to simulated positioning
   - Real-time location tracking

5. **Visual Feedback**
   - Dynamic dashboard updates across all tabs
   - Color-coded alert system
   - Historical data tracking and visualization

## ğŸ“ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¥ Team

- **Rohit Sagar** - _Initial work_ - [rohitsagar363](https://github.com/rohitsagar363)

---
<div align="center">
Made with â¤ï¸ for the AI Accessibility Hackathon
</div>

ğŸ“¸ Output Screenshots
ğŸŸ¢ Safe Detection Example

ğŸ”´ Alert Detection Example

ğŸ§  AI Insight Summary View

ğŸ§© Demo Flow
Launch the Streamlit app.

Click Start Listening.

Play sounds (e.g., music, speech, alarm) or use your voice.

Watch the live updates, AI summaries, and alert banners appear instantly.

ğŸ§­ Future Scope & Enhancements
Area	Next Steps
ğŸ”” IoT Integration	Deploy on Raspberry Pi / ESP32 for edge detection and visual beacons.
ğŸ§  LLM Summarization	Integrate Gemini or GPT-5 for emotion / urgency analysis of sound context.
ğŸ“¡ Multi-Mic Localization	Triangulate sound source direction with a 3-mic array.
ğŸŒ Cloud Dashboard	Push events to Firebase / MQTT for remote monitoring and alerts.
ğŸ“± Mobile App Companion	Send push notifications for critical sound events.
ğŸ¨ Accessibility Design	Add vibration / haptic feedback and color-blind themes.
ğŸ”’ Data Privacy	Fully on-device inference â€” no audio data leaves the system.
ğŸ§¾ Analytics Layer	Generate daily summaries and sound frequency heatmaps.

ğŸ‘¨â€ğŸ’» Team
Team: Udta Buffalo ğŸ¦¬
Member	Role
Rohith Sagar Karnala
Bhargav
Amal	
Manogna

ğŸ Hackathon Demo Highlights
Show real-time sound detection and floating alert banners.

Demonstrate AI-generated summaries from Mistral.

Explain the impact for accessibility and deaf users.

Conclude with your Future Scope roadmap â€” Edge AI + IoT + LLMs.

ğŸ§  Acknowledgments
Special thanks to:

Google TensorFlow Hub for YAMNet.

Ollama & Mistral for local LLM inference.

Streamlit for rapid dashboard development.

ğŸ Summary
ğŸ¯ Low-Latency Sound Disambiguator represents an accessible, intelligent, and privacy-respecting approach to real-time audio awareness.
Itâ€™s lightweight, responsive, and extendable â€” bridging AI, accessibility, and edge computing into one unified platform.

