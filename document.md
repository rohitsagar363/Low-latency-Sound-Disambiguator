# ğŸ§ Low-Latency Sou## ğŸš¨ Use Case Example: Police Siren Detection

<div align="center">

### Without Sound Disambiguator
![Without System](./assets/images/before_siren.png)
*A deaf person unable to hear approaching emergency vehicle sirens*

### With Sound Disambiguator
![With System](./assets## ğŸ Summary

The **Low-Latency Sound Disambiguator** represents an accessible, intelligent, and privacy-respecting approach to real-time audio awareness. It's lightweight, responsive, and extendable â€” bridging AI, accessibility, and edge computing into one unified platform..png)guator

> Real-Time Audio Intelligence Dashboard for Sound Awareness

[![Hackathon Track](https://img.shields.io/badge/Track-AI%20for%20Accessibility-blue)](https://github.com/rohitsagar363/Low-latency-Sound-Disambiguator)
[![Edge Intelligence](https://img.shields.io/badge/Technology-Edge%20Intelligence-green)](https://github.com/rohitsagar363/Low-latency-Sound-Disambiguator)

## ğŸ§© Overview

The **Low-Latency Sound Disambiguator** is an innovative real-time audio intelligence system designed to enhance accessibility and sound awareness. This project:

- ğŸ¯ Listens to live microphone input continuously
- ğŸ¤– Utilizes Google's **YAMNet** for precise sound detection
- ğŸ§  Leverages **Ollama (Mistral)** for intelligent sound interpretation
- ğŸ“Š Provides instant visual feedback through an intuitive dashboard

### Core Use Case

> Empowering deaf and hearing-impaired individuals by transforming critical sounds (alarms, sirens, shouts) into clear, real-time visual alerts, enhancing safety and environmental awareness.

## ï¿½ Use Case Example: Police Siren Detection

<div align="center">

### Without Sound Disambiguator
![Without System](/images/before_siren.png)
*A deaf person unable to hear approaching emergency vehicle sirens*

### With Sound Disambiguator
![With System](/images/with_siren.png)
*Real-time visual alert showing:*
- ğŸš“ **Detection**: Police siren detected
- ğŸ“ **Direction**: Coming from behind, ~100m away
- ğŸ”Š **Intensity**: High (Emergency vehicle approaching)
- âš ï¸ **Action Required**: Move to the side of the road

</div>

> **Note**: Create an `images` directory and add relevant screenshots or mockups of your system in action. The images above are placeholders - replace them with actual screenshots from your application showing the alert system responding to police sirens.

## ï¿½ğŸš€ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ” Real-time Classification | Advanced sound detection using YAMNet (TensorFlow Hub) |
| âš¡ Low-latency Processing | Efficient streaming with SoundDevice + async queue |
| ğŸ¤– AI Insights | Smart context interpretation via Ollama (Mistral) |
| ğŸ“¡ Direction Detection | Intelligent stereo microphone utilization with fallback simulation |
| ğŸš¦ Visual Alerts | Smart color-coding system: ğŸŸ¢ Safe â€¢ ğŸŸ¡ Neutral â€¢ ğŸ”´ Alert |
| ğŸ“Š Rich Dashboard | Comprehensive view with Live/History/Analytics/Insights tabs |
| ğŸ’¨ Lightweight Design | Quick local deployment with minimal resource usage |  

## ğŸ“Š Dashboard Interface

### ğŸ¯ Live Tab
![Live Dashboard](/images/live_tab.png)
*Real-time monitoring and detection interface*
- Sound classification with confidence levels
- Direction indicator with spatial awareness
- Color-coded alert banner system
- Live AI interpretations of detected sounds

### ğŸ“œ History Tab
![History View](/images/history_tab.png)
*Historical data and event tracking*
- Chronological log of detected sounds
- Time-stamped events with classifications
- Filter and search functionality
- Export capabilities for analysis

### ğŸ“ˆ Analytics Tab
![Analytics Dashboard](/images/analytics_tab.png)
*Statistical analysis and insights*
- Sound type distribution charts
- Temporal pattern analysis
- Alert frequency statistics
- Performance metrics visualization

### ğŸ§  Insights Tab
![AI Insights](/images/insights_tab.png)
*AI-powered interpretation and recommendations*
- Contextual sound interpretations
- Pattern recognition summaries
- Environmental safety scoring
- Actionable safety recommendations

> **ğŸ“¸ Screenshots:** Place your application screenshots in `/images/`:
> - `live_tab.png` - Main monitoring interface
> - `history_tab.png` - Historical data view
> - `analytics_tab.png` - Statistical analysis view
> - `insights_tab.png` - AI interpretation view
> - `before_siren.png` - Use case without system
> - `with_siren.png` - Use case with system active

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TD
    A[ğŸ¤ Microphone Input] --> B[SoundDevice Stream]
    B --> C[YAMNet Model]
    C --> D{Sound Classification}
    D -->|Confidence & Label| E[Live Dashboard]
    D -->|Events| H[Analytics Engine]
    E --> F[Ollama Mistral]
    F --> G[Insights Generation]
    H --> I[Historical Data]
    E --> J[Alert System]
    J -->|Status| K[Visual Indicators]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#bbf,stroke:#333,stroke-width:2px
    style J fill:#fbb,stroke:#333,stroke-width:2px
```

## ğŸ§° Technology Stack

| Layer | Components | Description |
|-------|------------|-------------|
| ğŸ¨ Frontend | Streamlit, Plotly | Interactive dashboard with real-time updates |
| ğŸµ Audio | SoundDevice, NumPy | High-performance audio stream processing |
| ğŸ¤– ML/AI | TensorFlow Hub, YAMNet | Sound classification and analysis |
| ğŸ§  Intelligence | Ollama (Mistral) | Local LLM for context interpretation |
| ğŸ”„ Processing | Threading, Queue | Concurrent operation handling |
| ğŸ“Š Visualization | Plotly, Custom CSS | Dynamic charts and alert banners |

## âš™ï¸ Installation & Setup

### 1. Clone the Repository

```bash
git clone https://github.com/rohitsagar363/Low-latency-Sound-Disambiguator.git
cd Low-latency-Sound-Disambiguator
```

### 2. Set Up Python Environment

```bash
# Create virtual environment
python3 -m venv venv

# Activate virtual environment
# For Unix/macOS:
source venv/bin/activate
# For Windows:
# venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

#### Required Packages
```plaintext
streamlit>=1.24.0
tensorflow>=2.12.0
tensorflow_hub>=0.14.0
sounddevice>=0.4.6
numpy>=1.23.5
plotly>=5.15.0
requests>=2.31.0
pandas>=2.0.3
```

### 4. Configure Ollama

1. Download Ollama from [ollama.ai](https://ollama.ai)
2. Start the Ollama service:
```bash
# Pull the Mistral model
ollama pull mistral

# Start the Ollama service
ollama serve
```

### 5. Launch the Dashboard

```bash
streamlit run sound_alert_appv3.py
```

> ğŸ¤ **Note:** Grant microphone access when prompted by your system.
ğŸ”Š Grant microphone access when prompted.

## ğŸ§  System Operation

### Real-time Processing Pipeline

1. **Audio Capture**
   - Continuous streaming in 1.5-second chunks
   - Overlapping segments for smooth analysis
   - Real-time buffer management

2. **Sound Classification**
   - YAMNet model processes each chunk
   - Identifies sound categories and confidence levels
   - Low-latency inference optimization

3. **AI Interpretation**
   - Ollama (Mistral) analyzes classification results
   - Generates contextual two-line summaries
   - Provides human-readable insights

4. **Spatial Analysis**
   - Stereo microphone direction estimation
   - Fallback to simulated positioning
   - Real-time location tracking

5. **Visual Feedback**
   - Dynamic dashboard updates across all tabs
   - Color-coded alert system
   - Historical data tracking and visualization

## ğŸ“ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¥ Team

- **Rohit Sagar** - _Initial work_ - [rohitsagar363](https://github.com/rohitsagar363)

---
<div align="center">
Made with â¤ï¸ for the AI Accessibility Hackathon
</div>

ğŸ“¸ Output Screenshots
ğŸŸ¢ Safe Detection Example

ğŸ”´ Alert Detection Example

ğŸ§  AI Insight Summary View

ğŸ§© Demo Flow
Launch the Streamlit app.

Click Start Listening.

Play sounds (e.g., music, speech, alarm) or use your voice.

Watch the live updates, AI summaries, and alert banners appear instantly.

ğŸ§­ Future Scope & Enhancements
Area	Next Steps
ğŸ”” IoT Integration	Deploy on Raspberry Pi / ESP32 for edge detection and visual beacons.
ğŸ§  LLM Summarization	Integrate Gemini or GPT-5 for emotion / urgency analysis of sound context.
ğŸ“¡ Multi-Mic Localization	Triangulate sound source direction with a 3-mic array.
ğŸŒ Cloud Dashboard	Push events to Firebase / MQTT for remote monitoring and alerts.
ğŸ“± Mobile App Companion	Send push notifications for critical sound events.
ğŸ¨ Accessibility Design	Add vibration / haptic feedback and color-blind themes.
ğŸ”’ Data Privacy	Fully on-device inference â€” no audio data leaves the system.
ğŸ§¾ Analytics Layer	Generate daily summaries and sound frequency heatmaps.

ğŸ‘¨â€ğŸ’» Team
Team: Udta Buffalo ğŸ¦¬
Member	Role
Rohith Sagar Karnala	System Architecture, Streamlit Dashboard, ML Integration
Bhargav	Backend Logic, Audio Processing, Model Integration
Amal	Frontend Design, Visualization, Accessibility Enhancements
Manogna	AI Summarization, Testing, Hackathon Presentation

ğŸ Hackathon Demo Highlights
Show real-time sound detection and floating alert banners.

Demonstrate AI-generated summaries from Mistral.

Explain the impact for accessibility and deaf users.

Conclude with your Future Scope roadmap â€” Edge AI + IoT + LLMs.

ğŸ§  Acknowledgments
Special thanks to:

Google TensorFlow Hub for YAMNet.

Ollama & Mistral for local LLM inference.

Streamlit for rapid dashboard development.

ğŸ Summary
ğŸ¯ Low-Latency Sound Disambiguator represents an accessible, intelligent, and privacy-respecting approach to real-time audio awareness.
Itâ€™s lightweight, responsive, and extendable â€” bridging AI, accessibility, and edge computing into one unified platform.


## ğŸš€ Project Overview  
The **Low-Latency Sound Disambiguator** is a **real-time AI-powered sound awareness dashboard** designed to help **deaf and hearing-impaired users**.  
It detects, classifies, and interprets sounds instantly using:  
- **Googleâ€™s YAMNet (TensorFlow Hub)** for sound classification.  
- **Ollama (Mistral LLM)** for AI-based contextual summaries.  
- **Streamlit** for dynamic visualization and alert banners.

**ğŸ† Hackathon Track:** AI for Accessibility / Edge Intelligence  
**ğŸ¯ Goal:** Convert environmental sounds into intelligent visual alerts for rapid human response.

---

## âš™ï¸ System Architecture
```mermaid
graph TD
A[ğŸ¤ Microphone Input] --> B[SoundDevice Stream]
B --> C[YAMNet Classifier (TFHub)]
C --> D{Detected Sound Label}
D -->|Confidence| E[Streamlit Live Dashboard]
E --> F[Ollama (Mistral) Contextual AI]
F --> G[ğŸ§  Insights & Alerts]
E --> H[ğŸ“Š History + Analytics Tabs]
G --> I[ğŸ”´ Floating Alert Banner]
ğŸ’¡ Key Features
âš¡ Ultra-low latency detection pipeline (1.5s sliding window).

ğŸ§© AI-driven contextual summaries for every detected sound.

ğŸŸ¢ğŸŸ¡ğŸ”´ Color-coded alerts: Safe / Neutral / Emergency.

ğŸ§ Stereo microphone support for true directional awareness.

ğŸ“ˆ Live analytics dashboard showing confidence, direction, amplitude.

ğŸ’» Completely local inference â€” private and offline.

ğŸ–¥ï¸ Dashboard Interface
ğŸ”´ Real-Time Detection

ğŸ§  AI Summaries & Context

ğŸŸ¢ Normal Operation

ğŸ§° Technology Stack
Category	Tool
ML Model	TensorFlow Hub â€“ YAMNet
AI Reasoning	Ollama (Mistral LLM)
Dashboard	Streamlit, Plotly
Audio Ingestion	SoundDevice, NumPy
Processing	Threading + Async Queues
Deployment	Python 3.11, Localhost

âš¡ Setup Guide
bash
Copy code
# Clone & Setup
git clone https://github.com/<your-handle>/Low-latency-Sound-Disambiguator.git
cd Low-latency-Sound-Disambiguator
pip install -r requirements.txt

# Start Ollama
ollama pull mistral
ollama serve

# Run App
streamlit run sound_alert_appv3.py
ğŸ§© Demonstration Flow
Start listening via the Streamlit UI.

Generate sounds (music, alarms, voices).

Observe instant classification, AI insight, and floating alert banner.

Review detection history and analytics trends.

ğŸ“¡ Future Enhancements
Area	Next Step
ğŸ”” IoT Integration	Deploy on Raspberry Pi for edge alerts.
ğŸ“± Mobile Companion	Push notifications for critical events.
ğŸ™ï¸ Multi-Mic Array	Triangulate true sound direction.
ğŸ§  Emotion Detection	Use LLMs to assess urgency and tone.
ğŸŒ Cloud Extension	Real-time alert dashboard with MQTT.
ğŸ’¬ Haptic Feedback	Wearable integration for vibration alerts.

ğŸ‘¨â€ğŸ’» Team: Udta Buffalo ğŸ¦¬
Member
Rohith Sagar Karnala	
Bhargav
Amal
Manogna

ğŸ Hackathon Highlights
âš™ï¸ Fully functional end-to-end local demo.

ğŸ’¡ AI-generated live summaries and context.

ğŸ“Š Real-time dashboard with analytics and insights.

ğŸŒ Accessibility-focused impact for deaf users.

ğŸ§  Impact
The Low-Latency Sound Disambiguator demonstrates how AI and accessibility can merge to improve everyday safety and awareness â€” empowering those with hearing impairments through instant, intelligent, and interpretable sound recognition.

ğŸ† Acknowledgments
Google TensorFlow Hub â€“ YAMNet model.

Ollama (Mistral) â€“ local LLM context generation.

Streamlit â€“ dashboard framework.

University at Buffalo Hackathon Team â€“ Udta Buffalo ğŸ¦¬
